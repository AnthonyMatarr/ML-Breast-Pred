{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159a4cca",
   "metadata": {},
   "source": [
    "# Feature Importance (SHAP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9abd22",
   "metadata": {},
   "source": [
    "## Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f7dc6c",
   "metadata": {},
   "source": [
    "Packages/Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9004221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src.data_utils import get_data, get_models, get_feature_lists\n",
    "from src.config import BASE_PATH, SEED\n",
    "from src.feat_importance import get_shap_single_model\n",
    "from joblib import delayed, Parallel\n",
    "from src.nn_model import load_nn_clf\n",
    "from shutil import rmtree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "\n",
    "shap.utils._general._show_progress = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d066446f",
   "metadata": {},
   "source": [
    "Set Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67bfbfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "file_dir = BASE_PATH / \"data\" / \"processed\"\n",
    "OUTCOME_DICT = {\n",
    "    \"med\": get_data(\"med_outcome\", file_dir),\n",
    "    \"surg\": get_data(\"surg_outcome\", file_dir),\n",
    "    \"mort\": get_data(\"mort_outcome\", file_dir),\n",
    "    \"reop\": get_data(\"reop_outcome\", file_dir),\n",
    "    \"vte\": get_data(\"vte_outcome\", file_dir),\n",
    "}\n",
    "\n",
    "# Models\n",
    "model_dir = BASE_PATH / \"models\" / \"trained\"\n",
    "model_prefix_list = [\"lgbm\", \"lr\", \"xgb\", \"stack\"]\n",
    "MODEL_DICT = {}\n",
    "X_shape = OUTCOME_DICT[\"surg\"][\"X_train\"].shape[1]  # same for all outcomes\n",
    "for outcome in OUTCOME_DICT.keys():\n",
    "    ## Base models\n",
    "    MODEL_DICT[outcome] = get_models(model_prefix_list, outcome, model_dir)\n",
    "    ## Neural network\n",
    "    nn_import = load_nn_clf(\n",
    "        data_path=BASE_PATH / \"models\" / \"trained\" / outcome / \"nn.pt\",\n",
    "        in_dim=X_shape,\n",
    "        device=\"cpu\",\n",
    "    )\n",
    "    MODEL_DICT[outcome][\"nn\"] = nn_import\n",
    "\n",
    "\n",
    "FEAT_ORDER = [\n",
    "    ##Demographics + Comorbidities\n",
    "    \"SURGINDICD\",\n",
    "    \"AGE\",\n",
    "    \"BMI\",\n",
    "    \"SEX\",\n",
    "    \"ETHNICITY_HISPANIC\",\n",
    "    \"RACE\",\n",
    "    \"DIABETES\",\n",
    "    \"HXCOPD\",\n",
    "    \"HXCHF\",\n",
    "    \"ASCITES\",\n",
    "    \"BLEEDDIS\",\n",
    "    \"TRANSFUS\",\n",
    "    \"DIALYSIS\",\n",
    "    \"HYPERMED\",\n",
    "    \"VENTILAT\",\n",
    "    \"SMOKE\",\n",
    "    \"DISCANCR\",\n",
    "    \"RENAFAIL\",\n",
    "    \"STEROID\",\n",
    "    \"ASACLAS\",\n",
    "    \"DYSPNEA\",\n",
    "    \"WNDINF\",\n",
    "    \"WTLOSS\",\n",
    "    ## Blood Labs\n",
    "    \"PRALBUM\",\n",
    "    \"PRWBC\",\n",
    "    \"PRHCT\",\n",
    "    \"PRPLATE\",\n",
    "    ## Intra-Op\n",
    "    \"OPTIME\",\n",
    "    \"URGENCY\",\n",
    "    \"ANESTHES\",\n",
    "    \"SURGSPEC\",\n",
    "    \"INOUT\",\n",
    "    \"OPERYR\",\n",
    "    ## Mastectomy\n",
    "    \"SNLBCPT\",\n",
    "    \"ALNDCPT\",\n",
    "    \"PARTIALCPT\",\n",
    "    \"SUBSIMPLECPT\",\n",
    "    \"RADICALCPT\",\n",
    "    \"MODIFIEDRADICALCPT\",\n",
    "    ## Reconstruction\n",
    "    \"IMMEDIATECPT\",\n",
    "    \"DELAYEDCPT\",\n",
    "    \"TEINSERTIONCPT\",\n",
    "    \"TEEXPANDERCPT\",\n",
    "    \"FREECPT\",\n",
    "    \"LATCPT\",\n",
    "    \"SINTRAMCPT\",\n",
    "    \"SINTRAMSUPERCPT\",\n",
    "    \"BITRAMCPT\",\n",
    "    \"AUGPROSIMPCPT\",\n",
    "    \"OTHERRECONTECHCPT\",\n",
    "    \"REVRECBREASTCPT\",\n",
    "    \"FATGRAFTCPT\",\n",
    "    \"ADJTISTRANSCPT\",\n",
    "    \"MASTOCPT\",\n",
    "    \"BREASTREDCPT\",\n",
    "    \"NPWTCPT\",\n",
    "]\n",
    "\n",
    "\n",
    "FEAT_ORDER = [str(col.upper()) for col in FEAT_ORDER]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0813103",
   "metadata": {},
   "source": [
    "Ensure we got all bases covered with feat_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c84308",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_df = OUTCOME_DICT[\"med\"][\"X_test\"][:5]\n",
    "all_cols = set()\n",
    "for col in dummy_df.columns:\n",
    "    col_split = col.split(\"_\")\n",
    "    if len(col_split) == 1 or col_split[0] == \"ETHNICITY\":\n",
    "        all_cols.add(col)\n",
    "    else:\n",
    "        col_name = col_split[0]\n",
    "        all_cols.add(col_name)\n",
    "assert set(FEAT_ORDER) == set(all_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d138e47",
   "metadata": {},
   "source": [
    "## RUN SHAP\n",
    "\n",
    "Run in parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7484e3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = []\n",
    "for outcome_name, outcome_data in OUTCOME_DICT.items():\n",
    "    save_dir = BASE_PATH / \"data\" / \"SHAP\" / outcome_name\n",
    "    X_train = outcome_data[\"X_train\"]\n",
    "    y_train = outcome_data[\"y_train\"]\n",
    "    X_test = outcome_data[\"X_test\"]\n",
    "    y_test = outcome_data[\"y_test\"]\n",
    "    #########################################################################################################\n",
    "    ################################Subset of data for testing workflow######################################\n",
    "    #########################################################################################################\n",
    "    # X_train, _, y_train, _ = train_test_split(\n",
    "    #     outcome_data[\"X_train\"],\n",
    "    #     outcome_data[\"y_train\"],\n",
    "    #     stratify=outcome_data[\"y_train\"],\n",
    "    #     random_state=SEED,\n",
    "    #     train_size=0.25,\n",
    "    # )\n",
    "    # _, X_test, _, y_test = train_test_split(\n",
    "    #     outcome_data[\"X_test\"],\n",
    "    #     outcome_data[\"y_test\"],\n",
    "    #     stratify=outcome_data[\"y_test\"],\n",
    "    #     random_state=SEED,\n",
    "    #     test_size=0.25,\n",
    "    # )\n",
    "    #########################################################################################################\n",
    "    #########################################################################################################\n",
    "    #########################################################################################################\n",
    "    # ================================> Save a subset for explanation\n",
    "    # X_test has ~110k, we want ~10k so use 10%\n",
    "    # LR, XGB, LGBM models are ~fast, can use more entries\n",
    "    _, X_explain_base, _, _ = train_test_split(\n",
    "        X_test, y_test, stratify=y_test, test_size=0.1, random_state=SEED, shuffle=True\n",
    "    )\n",
    "    # X_test has ~110k, we want ~5k so use 5%\n",
    "    # NN KernelExplainer() slow, use less patients\n",
    "    _, X_explain_nn, _, _ = train_test_split(\n",
    "        X_test, y_test, stratify=y_test, test_size=0.05, random_state=SEED, shuffle=True\n",
    "    )\n",
    "    # X_test has ~110k, we want ~1k so use 1%\n",
    "    # Stack KernelExplainer() extremely slow, use way less patients\n",
    "    _, X_explain_stack, _, _ = train_test_split(\n",
    "        X_test, y_test, stratify=y_test, test_size=0.01, random_state=SEED, shuffle=True\n",
    "    )\n",
    "    print(f\"Base size: {len(X_explain_base)}\")\n",
    "    print(f\"Kernel Size for NN: {len(X_explain_nn)}\")\n",
    "    print(f\"Kernel Size for stack: {len(X_explain_stack)}\")\n",
    "    # Save subset X_test\n",
    "    explain_save_path = save_dir / \"explain\"\n",
    "    if explain_save_path.exists():\n",
    "        rmtree(explain_save_path)\n",
    "    explain_save_path.mkdir(exist_ok=True, parents=True)\n",
    "    base_explain_path = explain_save_path / \"base.parquet\"\n",
    "    X_explain_base.to_parquet(base_explain_path)\n",
    "    nn_kernel_explain_path = explain_save_path / \"kernel.parquet\"\n",
    "    X_explain_nn.to_parquet(nn_kernel_explain_path)\n",
    "    stack_kernel_explain_path = explain_save_path / \"kernel_stack.parquet\"\n",
    "    X_explain_stack.to_parquet(stack_kernel_explain_path)\n",
    "    cur_model_dict = MODEL_DICT[outcome_name]\n",
    "    for model_name, model in cur_model_dict.items():\n",
    "        # ================================> Split X_train for background (per model)\n",
    "        if model_name != \"stack\":\n",
    "            continue\n",
    "        # NOTE: Train set ~515k patients\n",
    "        if model_name in [\"lr\", \"xgb\", \"lgbm\"]:\n",
    "            # Explainers are ~fast, so can have larger background set (~5000)\n",
    "            train_size = 0.01\n",
    "            explain_path = base_explain_path\n",
    "        elif model_name == \"nn\":\n",
    "            # Explainer slower, so less background (~50)\n",
    "            train_size = 0.0001\n",
    "            explain_path = nn_kernel_explain_path\n",
    "        elif model_name == \"stack\":\n",
    "            # Explainer slower, so less background (~50)\n",
    "            train_size = 0.0001\n",
    "            explain_path = stack_kernel_explain_path\n",
    "        else:\n",
    "            raise ValueError(\"Model not recognized\")\n",
    "        X_background, _, _, _ = train_test_split(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            stratify=y_train,\n",
    "            train_size=train_size,\n",
    "            random_state=SEED,\n",
    "            shuffle=True,\n",
    "        )\n",
    "        ##Write to memory\n",
    "        background_save_path = save_dir / f\"{model_name}_background.parquet\"\n",
    "        if background_save_path.exists():\n",
    "            background_save_path.unlink()\n",
    "        background_save_path.parent.mkdir(exist_ok=True, parents=True)\n",
    "        X_background.to_parquet(background_save_path)\n",
    "        print(f\"{outcome_name} background size: {len(X_background)}\")\n",
    "        # ================================> CALL SHAP\n",
    "        log_path = BASE_PATH / \"shap_logs\" / model_name / f\"{outcome_name}.log\"\n",
    "        if log_path.exists():\n",
    "            log_path.unlink()\n",
    "        result_path = BASE_PATH / \"results\" / \"tables\" / \"SHAP\"\n",
    "        if result_path.exists():\n",
    "            rmtree(result_path)\n",
    "        jobs.append(\n",
    "            delayed(get_shap_single_model)(\n",
    "                model=model,\n",
    "                model_name=model_name,\n",
    "                feat_order=FEAT_ORDER,\n",
    "                outcome_name=outcome_name,\n",
    "                explanation_path=explain_path,\n",
    "                background_path=background_save_path,\n",
    "                log_path=log_path,\n",
    "                result_path=result_path,\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a391609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run jobs with 25 parallel workers\n",
    "print(\"=== Starting jobs on CPU ===\")\n",
    "Parallel(n_jobs=min(25, len(jobs)), backend=\"loky\")(jobs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
